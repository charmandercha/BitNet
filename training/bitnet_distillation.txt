                                                                                                     BitNet Distillation


                                                         Xun Wu Shaohan Huang Wenhui Wang                                                  Ting Song Li Dong Yan Xia Furu Wei†

                                                                                                                 Microsoft Research
                                                                                                              https://aka.ms/GeneralAI

                                                                                                                                 Abstract
arXiv:2510.13998v1 [cs.LG] 15 Oct 2025




                                                               In this paper, we present BitNet Distillation (BitDistill), a lightweight
                                                               pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into 1.58-
                                                               bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream tasks,
                                                               achieving strong task-specific performance with minimal computational cost.
                                                               Specifically, BitDistill incorporates three key techniques: the SubLN mod-
                                                               ule, as introduced in BitNet [WMD+ 23]; multi-head attention distillation, based
                                                               on MiniLM [WBH+ 20]; and continual pre-training, which serves as a crucial
                                                               warm-up step to mitigate the scalability issue of the performance gap between
                                                               finetuned full-precision and 1.58-bit LLMs on specific tasks. Experimental re-
                                                               sults show that BitDistill achieves performance comparable to the full-
                                                               precision counterpart models across model size, while enabling up to 10×
                                                               memory savings and 2.65× faster inference on CPUs. Code is available at
                                                               github.com/microsoft/BitNet.


                                                                   Task Performance Comparison on                                                                      Inference Speed (CPUs)
                                                              Classification (Left) & Summarization (Right)                                                           and Memory Comparison
                                                                                             91.5 91.4
                                                                                                                                     27.6 27.4
                                                                              89.6 89.5
                                                              88.0 88.2                                                                                          1000                     1
                                                         88                                                                     27
                                                                                                                 BLEU & ROUGE
                                          Accuracy (%)




                                                                                                          15.4
                                                                                                                                                         Tokens / s




                                                                                                                                                                       2.65×                           10×
                                                                                                                                                                                    GB




                                                                                          14.3
                                                         83
                                                                          13.9
                                                                                                                                25                                    600             0.5

                                                         78                                                                                       24.0
                                                                                                       76.1
                                                                                      75.3
                                                                       74.1
                                                         73                                                                     23                                    200                 0
                                                                0.6B             1.7B            4B                                      0.6B                           Inference Speed        Memory

                                                                 FP16            1.58-bit BitDistill (Ours)                 1.58-bit BitNet-SFT                             FP16          1.58-bit BitNet



                                         Figure 1: Performance on downstream tasks across model size, with inference speed and mem-
                                         ory efficiency comparison. We observed that directly finetuning full-precision LLMs into 1.58-bit
                                         LLMs (denoted as 1.58-bit BitNet-SFT) leads to a notable performance gap compared to the FP16
                                         baseline, and this gap remains or even widens as the model size increases. In contrast, BitDistill
                                         preserves scalability, resulting in performance comparable to full-precision counterparts across all
                                         model size, while reducing 10× memory usage and 2.65× faster inference on CPUs.




                                                  †
                                                         Corresponding author.
1   Introduction
Large Language Models (LLMs) [AAA+ 23, GYZ+ 25] have become indispensable not only in ad-
vancing general natural language processing [YLY+ 25], but more importantly in powering a wide
range of downstream applications, such as recommendation [WZQ+ 24, HZL+ 24, RWX+ 24], clas-
sification [KDSP25, SLL+ 23], and retrieval [ZSB+ 24, BMH+ 22]. Despite their broad applicabil-
ity, deploying LLMs in downstream applications remains highly challenging. The rapid escalation
in model size further amplifies these challenges, especially on resource-constrained devices (e.g.,
smartphones), where both memory consumption and computational overhead become prohibitive.
To address these challenges, recent efforts on extreme low-bit LLMs, such as the 1.58-bit (i.e.,
ternary values {-1, 0, 1}) BitNet [MWM+ 24, MWH+ 25, WMD+ 23], aim to dramatically reduce
memory footprint and accelerate inference, offering a promising avenue for efficient deployment
in downstream applications. However, achieving competitive accuracy on downstream applications
with 1.58-bit BitNet generally requires pretraining from scratch on large-scale corpora [TXL+ 25,
MWH+ 25] first, resulting in substantial computational and energy overhead. Furthermore, as il-
lustrated in Figure 1, directly applying quantization-aware training (QAT) [DZC+ 24, CSX+ 24] to
existing full-precision LLMs at 1.58-bit for specific downstream tasks is often unstable, fails to
fully preserve the performance of their full-precision counterparts, and exhibit an poor scalability:
as model size increases from 0.6B to 4B, the performance gap relative to the full-precision base-
line grows from 13.9 to 15.3. This highlights the pressing need for more effective QAT methods
specifically designed for 1.58-bit BitNet.
In this work, we focus on fine-tuning existing LLMs to 1.58-bit for specific downstream tasks,
while achieving performance comparable to their full-precision counterparts. To this end,
we propose BitNet Distillation (BitDistill), a scaling-friendly QAT framework designed
to bridge the gap between extreme 1.58-bit quantization and practical deployment. BitDistill
comprises three stages: (i) modeling refinement with SubLN module [WMD+ 23] for stable opti-
mization, (ii) continued pre-training to mitigate scale-related performance gaps, and (iii) MiniLM-
based [WWD+ 20, WBH+ 20] multi-head attention distillation to recover full-precision accuracy.
Through extensive evaluations across four benchmarks and diverse model scales, we demonstrate
that BitDistill scales effectively, achieving downstream task performance on par with full-
precision baselines. At the same time, as shown in Figure 1, it reduces 10× memory savings and
2.65× faster inference on CPUs, offering significant improvements in latency, throughput, mem-
ory efficiency, and energy consumption, which makes it particularly well-suited for deployment on
resource-constrained hardware.
Specifically, this work makes the following contributions:
1. To the best of our knowledge, we are the first to investigate fine-tuning pre-trained full-precision
   LLMs into 1.58-bit BitNet for specific downstream tasks, and we identify key challenges includ-
   ing: performance degradation, poor scalability, and training instability.
2. To address these challenges, we propose a tailored distillation framework named BitDistill,
   which comprises three key techniques: the SubLN module, as introduced in BitNet [WMD+ 23];
   multi-head attention distillation, based on MiniLM [WBH+ 20]; and continual pre-training,
   which serves as a crucial warm-up step to mitigate the scalability issue of the performance gap
   between finetuned full-precision and 1.58-bit LLMs on specific tasks.
3. Extensive experiments across multiple benchmarks and model scales show that BitDistill
   enables 1.58-bit quantized LLMs to achieve downstream performance comparable to their full-
   precision counterparts, while enabling up to 10× memory savings and 2.65× faster inference on
   CPUs.

2   Preliminaries
1.58-bit Quantization. Following [MWM+ 24], we adopt per-tensor quantization using the
absmean function to map the weights of existing LLMs into ternary values, i.e., {-1, 0, 1}:
                                                  WFP16
                       Qw (W) = ∆ RoundClip(            , −1, 1),                           (1)
                                                  ∆+ϵ
            where ∆ = mean(|W|), RoundClip(Y, a, b) = min (max (⌊Y⌉, a) , b) ,              (2)


                                                  2
The notation ⌊·⌉ means the nearest rounding operation. For LLM inputs, we employ 8-bit activa-
tion quantization. Specifically, we use per-token absmax and absmean functions to quantize the
activations, which can be formulated as:
                            γ             127
            QINT8 (X) =        RoundClip(     XFP16 , −128, 127), γ = max(|XFP16 |)                  (3)
                           127            γ+ϵ
Gradient Approximation. Due to the presence of non-differentiable operations in Eq. 2 and Eq. 3
(e.g., RoundClip), the gradient cannot be propagated through the entire model during backpropa-
gation. Following [MWM+ 24, MWH+ 25, WMD+ 23], we employ the Straight-Through Estimator
(STE) [BLC13] to approximate gradients for 1.58-bit quantized LLMs.

3     BitDistill: Finetuning LLMs into 1.58-bit BitNet For Downstream
      Tasks
In this work, we address the challenge of deploying LLMs on resource-constrained devices for
specific downstream tasks. We focus on efficiently compressing existing pre-trained LLMs to
1.58-bit BitNet with minimal performance degradation and training cost. Our proposed BitNet
Distillation (BitDistill) incorporates three key stages: (1) Modeling refinement with
SubLN [WMD+ 23] for stable optimization (detailed in § 3.1), (2) Continue pre-training as a
crucial warm-up step to mitigate the performance gap that does not scale well between fine-tuned
full-precision models and 1.58-bit BitNet (see in § 3.2), and (3) Distillation-based fine-tuning,
which leverages both logits distillation and multi-head attention distillation to recover full-precision
performance (see §3.3).

3.1   Stage-1: Modeling Refinement

Unlike full-precision models, where the variance of hidden states is typically preserved within a
stable range under standard initialization schemes, low-bit quantized models such as 1.58-bit LLMs
often suffer from excessively large activation variance, which results in optimization instability and
degraded convergence [MWM+ 24, WMD+ 23].
To alleviate this issue, following the design principles of prior 1.58-bit BitNet [MWM+ 24,
MWH+ 25], we introduce additional normalization layers named SubLN at carefully chosen po-
sitions inside each transformer block. Specifically, instead of only applying pre-normalization at
the block input, we further insert SubLN right before the output projection of the Multi-Head Self-
Attention (MHSA) module as well as before the output projection of the Feed-Forward Network
(FFN). Concretely, taking Qwen3 [YLY+ 25] as a reference architecture, the computations at the
l-th transformer layer are modified as:
                                                               MHSA
                            Yl = Xl + SubLN Concat(heads) Wout          ,                      (4)
                                                  FFN              FFN
                                                                        FFN
                    Xl+1 = Yl + SubLN ( Yl Wup ) ⊙ σ(Yl Wgate ) Wdown ,                        (5)
where
                     Qi K ⊤
           n                                                           o
                          i             MHSA          MHSA          MHSA
    heads = Softmax   √       Vi Qi = XWQ,i  , Ki = XWK,i  , Vi = XWV,i   , (6)
                        d

where the outer SubLN in each equation corresponds to the newly inserted normalization before the
respective output projection. This design ensures that the hidden representations entering quantized
projection layers are variance-stabilized, preventing the explosion of activation scale and thereby
improving both training stability and task performance.

3.2   Stage-2: Continue Pre-Training

As shown in Figure 1, directly fine-tuning 1.58-bit BitNet modified from existing full-precision
LLMs on downstream tasks may yield suboptimal results, as the limited number of training tokens
is often insufficient to effectively adapt full-precision weights into the constrained 1.58-bit represen-
tation, which leads to exhibit poor scalability: as model size increases, the performance gap relative
to the full-precision baseline widens.


                                                   3
To this end, we propose a two-stage training pipeline consisting of a continue training stage, which
leverages only a small amount of pretraining corpus to achieve the desired adaptation, followed by
fine-tuning on the downstream task. Specifically, given a small set of corpus C = {c1 , · · · , cN },
we finetuning the modeling-modified pre-trained LLMs attained from § 3.1 as:
                                           N Ti
                                        1 XX
                              LCT = −             log Pθ (ci,t | ci,<t ) .                          (7)
                                        N i=1 t=1
Here Pθ denotes the probability distribution parameterized by the model. A detailed analysis of
the effect of continue training, along with an investigation into the underlying mechanisms and
supporting hypotheses, can be found in § 4.4.

3.3   Stage-3: Distillation-based Fine-tuning

To better mitigate the performance degradation introduced by precision reduction, we incorporate
two kinds of knowledge distillation technology into the downstream task finetuning phase, where
the fine-tuned full-precision LLMs serves as the teacher and its 1.58-bit quantized counterpart acts
as the student.
Logits Distillation. Logits distillation has recently been widely adopted in the QAT phase of quan-
tized models, demonstrating promising effectiveness [DZC+ 24, LSK+ 25, KKCY24]. Given data
pairs {(xi , yi )}N
                  i=1 sampled from downstream datasets, the objective of logits distillation is defined
as
                                 N
                              1 X
                                    DKL PθFP16 (yi | xi )     Pθ1.58-bit (yi | xi ) ,
                                                                                   
                      LLD =                                                                         (8)
                              N i=1
                                 (·)              exp(zy (x; θ)/τ )
                                Pθ (y | x) = P                                                      (9)
                                                  y ′ exp(zy (x; θ)/τ )
                                                            ′


Here zy (x; θ) denotes the unnormalized logit produced by the model for y when given the input x.
The temperature parameter τ is introduced to control the softening of the output distributions for
both the FP16 and 1.58-bit models. DKL (· ∥ ·) represents the Kullback–Leibler divergence.
Multi-Head Attention Distillation,. Since the attention mechanism plays a pivotal role in LLMs
and largely determines their overall performance, we further investigate distillation at the attention
layer to encourages the 1.58-bit student to capture the fine-grained structural dependencies embed-
ded in the FP16 teacher’s attention patterns.
Following MiniLM series [WBH+ 20, WWD+ 20], given training samples x drawn from the down-
stream dataset, we define the attention-relations distillation loss LAD as
                                    A(·) ∼ Φ,    Φ = {Q, K, V},                                    (10)
                               |Υ| |Φ|         Ar X|x|
                            1 XX          1 X
                   LAD =               αi              DKL (RFP16       1.58-bit
                                                             i,j,a,t ∥ Ri,j,a,t ).                 (11)
                           |Υ| i=1 j=1 Ar |x| a=1 t=1

Here Φ correspond to the query, key, and value projections within a multi-head attention block, and
Υ denotes the set of layers we selected for distillation. αi are coefficients controlling the relative
weights of different relational terms. The sequence length is denoted by |x|, Ar is the number
                                                   (·)
of attention heads. The relational distribution Ri,j,a,t is derived by applying scaled dot-product
attention followed by Softmax with hidden dimension dr , while R1.58-bit
                                                                      i,j,a,t is obtained analogously
from the quantized student model using hidden dimension d′r , i.e.,
                                     FP16 ⊤                                          1.58-bit ⊤
                                             !                                                  !
      FP16
                          AFP16
                            i,j,a,t Ai,j,a,t       1.58-bit
                                                                         A1.58-bit
                                                                            i,j,a,t Ai,j,a,t
    Ri,j,a,t = Softmax           √             , Ri,j,a,t = Softmax              p                . (12)
                                    dr                                              d′r

The detailed implement of LAD can be found in Algorithm 1. Following MiniLM [WWD+ 20,
WBH+ 20], we recommend performing attention distillation at only a single layer (i.e., |Υ| = 1)
rather than across all layers, as conferring greater optimization flexibility to the 1.58-bit student
BitNet often yields superior downstream performance.


                                                   4
Algorithm 1 Pseudo Torch Style Implement of LAD
def compute_attention_distillation_loss(student_states, teacher_states, distill_layer, split_heads):
  # student_states [3, B, num_heads, seq_len, head_dim]: Q, K, V states from the 1.58-bit model
  # teacher_states [3, B, num_heads, seq_len, head_dim]: Q, K, V states from the FP16 model
  # distill_layer [1]: the index of layers used for distillation
  # split_heads [1]: the number of heads when computing attention relation matrix
  _, B, heads, L, d = student_states.shape
  D = heads * d // split_heads
  # Loop for computing distillation loss across Q, K, V
  for i in range(3):
    s_values, t_values = student_states[i], teacher_states[i]
    s_values = F.normalize(s_values.transpose(1, 2).reshape(B, L, split_head, D).transpose(1, 2), dim=-1)
    t_values = F.normalize(t_values.transpose(1, 2).reshape(B, L, split_head, D).transpose(1, 2), dim=-1)
    # Compute relation martix
    s_relation = torch.matmul(s_values, s_values.transpose(-2, -1))
    t_relation = torch.matmul(t_values, t_values.transpose(-2, -1))
    # Reshape: [B, split_heads, L, L] -> [B*split_heads*L, L]
    s_relation = (s_relation / temperature).reshape(-1, L)
    t_relation = (t_relation / temperature).reshape(-1, L)

      s_prob = F.softmax(s_relation, dim=-1).clamp(min=1e-8)
      t_prob = F.softmax(t_relation, dim=-1).clamp(min=1e-8)

      distill_loss += F.kl_div(torch.log(s_prob), t_prob, reduction="batchmean", log_target=False)
    return distill_loss




The total loss of the distillation-based finetuning phase L comprises three terms that aim to minimize
the discrepancy between the student and teacher models and improve downstream task performance,
scaled by two distillation coefficients, λ and γ, i.e.,
                                             L = LCE + λLLD + γLAD ,                                  (13)
                                                            |yi |
                                                          N X
                                                      1   X
                                                                    log Pθ yit | xi .
                                                                                   
                                  where     LCE = −                                                   (14)
                                                      N   i=1 t=1

Here LCE denotes the cross-entropy loss on the downstream dataset. λ and γ control the trade-off
between distillation and model fitting.

4         Experiments
4.1        Experimental Setup

Datasets. We evaluate the effectiveness of our proposed method, BitDistill, on two rep-
resentative tasks: text classification and text summarization. For classification, we adopt
three widely used datasets from the General Language Understanding Evaluation (GLUE) bench-
mark [WSM+ 19]* : the Multi-Genre Natural Language Inference Corpus (MNLI) [WNB18], the
Question-answering Natural Language Inference dataset (QNLI) [RZLL16], and the Stanford Sen-
timent Treebank (SST-2) [SPW+ 13]. These datasets are employed for both training and evaluation
to comprehensively assess the effectiveness of our approach. For summarization, we use the CNN/-
DailyMail dataset (CNNDM) [HKG+ 15]† as both the training and evaluation corpus.
Baselines for Comparison. Since our objective is to fine-tune pre-trained full-precision LLMs into
1.58-bit BitNet models for specific downstream tasks, we compare the performance of our 1.58-bit
models (denoted as BitDistill) with that of FP16 models fine-tuned directly on the corresponding
downstream tasks (named FP16-SFT). In addition, we also report the results of directly converting
full-precision LLMs into 1.58-bit BitNet models and fine-tuning them on downstream tasks (denoted
as BitNet-SFT).
Training Settings. We fine-tune the Qwen3 series [YLY+ 25] as our base models, covering
0.6B, 1.7B, and 4B parameter scales. In addition, we investigate the impact of different base
model types by conducting experiments with alternative backbones such as Gemma [TKF+ 25] and
Qwen2.5 [QY+ 25]. For all baseline methods and our approach, we adopt a greedy search strategy
to select the optimal learning rate and training epochs. This procedure mitigates overfitting while
      * https://gluebenchmark.com/
      †
          https://huggingface.co/datasets/abisee/cnndailymail


                                                           5
Table 1: Results on text classification tasks. All models are initialized from the Qwen3 se-
ries [QY+ 25]. The top scores for each metric and dataset are highlighted in bold. The 1.58-bit
BitDistill models achieve performance comparable to the FP16 baseline while providing 2×
faster inference and 10× memory reduction across all datasets. ∗ denotes the FP16 teacher used in
BitDistill.
                                  MNLI                    QNLI                    SST2              Speed        Memory
 Method
                          0.6B    1.7B     4B     0.6B    1.7B     4B      0.6B   1.7B     4B     (tokens / s)    (G)
 FP16-SFT ∗               88.01   89.61   91.48   93.72   95.00   96.02   94.21   95.43   96.57          427      1.20
 BitNet-SFT               74.09   75.27   76.11   78.32   79.54   79.97   79.92   81.37   82.07         1,135     0.11
 BitDistill (Ours)        88.17   89.53   91.40   93.66   94.82   95.93   94.30   95.26   96.47         1,135     0.11


Table 2: Results on text summarization tasks (CNNDM dataset). All models are initialized from
the Qwen3 series [QY+ 25]. The top scores for each metric and dataset are highlighted in bold. The
1.58-bit BitDistill models achieve performance comparable to the FP16 baseline while providing
2× faster inference and 10× memory reduction across all datasets. ∗ denotes the FP16 teacher used
in BitDistill.
 Method                 BLEU ROUGE-1 ROUGE-2 ROUGE-L ROUGE-SUM AVG Speed (tokens / s) Memory (G)
                ∗
 FP16-SFT          13.98          40.62      17.77        27.72         37.80     27.58          427             1.20
 BitNet-SFT        11.47          37.10      13.97        24.84         33.37     24.15         1,135            0.11
 BitDistill (Ours) 14.41          40.21      17.47        27.49         37.63     27.44         1,135            0.11



ensuring both strong downstream performance and fair comparisons across methods. We fix the
maximum training sequence length to 512 tokens and the batch size to 32. All models are trained
on servers equipped with 8×AMD Mi300X GPUs.
Specifically, we set the temperature for logits distillation (Eq. 9) to 5.0. For the classification task,
we use λ = 10 and γ = 1e5 in Eq. 14, while for the summarization task, we set λ = 1 and
γ = 1e3. We set αi = 1.0 for all experiments. During the continue pre-training phase de-
scribed in §3.2, we further train our models using only 10B tokens sampled from the FALCON
corpus [PMH+ 23]. Compared with the cost of pre-training a 1.58-bit BitNet from scratch (approxi-
mately 4T tokens) [MWH+ 25], this additional cost is virtually negligible.
Evaluation Settings. For both classification and summarization task, we fix the sampling parame-
ters by setting top-p to 1.0 and the temperature to 0. Classification performance is evaluated using
accuracy. For the summarization task, we set the maximum generation length to 4096 tokens. Sum-
marization quality is assessed using BLEU [PRWZ02] and ROUGE-1, ROUGE-2, ROUGE-L and
ROUGE-SUM [Lin04]. For model runtime efficiency, we report the token throughput (tokens per
second) on CPU with 16 threads.

4.2        Main Results

Overall Performance. The overall evaluation results on the benchmark datasets are reported in
Table 1 and Table 2. Across different model sizes and tasks, the proposed 1.58-bit BitNet models
trained with our distillation framework (BitDistill) demonstrate accuracy that is largely compa-
rable to their full-precision counterparts, with only marginal differences observed in most cases. At
the same time, the 1.58-bit models deliver substantial gains in system efficiency, including up to a
2× inference speedup on CPUs and nearly an order-of-magnitude reduction in memory footprint.
These improvements underline the practical utility of our approach for scenarios where computa-
tional resources are constrained, while also showing that aggressive quantization can be made viable
with carefully designed distillation strategies.
Robustness to Different Pretrained Models. To further examine the generality of our frame-
work, we extend the evaluation by replacing the Qwen3 series with alternative base models such
as Qwen2.5 [QY+ 25]‡ and Gemma [TKF+ 25]§ . The results, summarized in Table 3, indicate that
BitDistill consistently yields downstream performance close to that of full-precision fine-tuning
      ‡
          https://huggingface.co/Qwen/Qwen2.5-0.5B
      §
          https://huggingface.co/google/gemma-3-1b-pt


                                                             6
Table 3: Results on the text classification task       Table 4: Results on the text classification task
(MNLI dataset) with different base model ini-          with different quantization techniques. B, G,
tializations. ∗ denotes the FP16 teacher used in       A indicates Block Quant, GPTQ and AWQ, re-
BitDistill.                                            spectively.

  Method         Gemma3-1B      Qwen2.5-0.5B             Method                      MNLI      QNLI

  FP16-SFT ∗         89.77           79.91               BitDistill                   88.17    93.66
                                                         BitDistill-B [DLSZ21]        88.23    93.74
  BitNet-SFT         78.02           60.80               BitDistill-G [FAHA22]        88.05    93.63
  BitDistill         89.61           79.98               BitDistill-A [LTT+ 24]       88.25    93.70



across all examined architectures. While minor performance fluctuations are observed between base
models, the trend remains stable, suggesting that our method is not tailored to a specific pretraining
family but can be applied more broadly. This robustness enhances the potential applicability of our
approach in diverse deployment environments, where the choice of pretrained backbone may vary
depending on availability and task requirements.

4.3   Ablation Study

Effect of each individual stages in BitDistill. As outlined in §3, the BitDistill framework
consists of three stages. To understand the contribution of each component, we conduct an ablation
study by removing one stage at a time and re-training the model. The results, reported in Table 5,
show that excluding any stage consistently leads to a non-trivial drop in downstream performance.
This suggests that each stage plays a complementary role, and that the full pipeline is necessary to
obtain the best trade-off between efficiency and accuracy.
Effect of different distillation techniques in Stage-3 §3.3. In the final stage of our framework,
we introduce two complementary distillation techniques to better optimize 1.58-bit BitNet models
for downstream tasks. To disentangle their respective effects, we compare using each technique
individually against the joint application of both. As shown in Table 6, while each technique alone
provides partial improvements, the combination leads to the most consistent performance across
benchmarks. This observation provides evidence that the two techniques address different aspects of
the optimization challenge, and their synergy is particularly beneficial under extreme quantization.
Compatibility with different quantization techniques. We further examine the compatibility of
BitDistill with existing post-training and weight-quantization approaches. In particular, we con-
sider Block-Quant [DLSZ21], GPTQ [FAHA22], AWQ [LTT+ 24], as well as the simple min–max
quantization scheme in Eq. 2. To this end, we integrate BitDistill with each quantization method
and evaluate the resulting 1.58-bit models. The results are summarized in Table 4 and lead to two
main observations: (1) regardless of the underlying quantization method, models benefit consistently
from the proposed framework and generally match the full-precision baseline, and (2) more sophis-
ticated quantization strategies (e.g., GPTQ, AWQ) provide additional gains on top of our distillation
pipeline. These findings suggest that BitDistill is complementary to different quantization algo-
rithms, offering a unified procedure that can stably enhance low-bit models across a diverse range
of quantization settings.

4.4   Analysis

Effect of SubLN used in Stage-1 § 3.1. To validate the effect of SubLN, we quantize existing
LLMs into 1.58-bit BitNet and fine-tune them on FALCON corpus, comparing the performance
with (denoted as BitNet-SFT w/ SubLN) and without the insertion of SubLN (denoted as BitNet-
SFT w/o SubLN). Specifically, as shown by the training loss curve in Figure 3 (a), we find that the
modeling refinement detailed in Stage-1 § 3.1, which modifies the LLMs’ architecture by inserting
SubLN layers at specific positions, effectively stabilizes the optimization of the 1.58-bit BitNet and
leads to improved performance.
Why continue-training mitigates the scalability issue. As stated in § 1, a critical challenge in ap-
plying 1.58-bit BitNet to downstream tasks is the poor scalability, i.e., as model size increases, the
performance gap between the 1.58-bit BitNet and its FP16 counterpart becomes increasingly pro-


                                                   7
  Attention-K   Attention-Q    Attention-V    Attention-O    FFN-Gate        FFN-Up       FFN-Down
Figure 2: Visualization of model weights. The top two rows show the quantized weights of BitNet
trained from scratch along with their corresponding FP16 distributions. The bottom two rows show
the quantized weights of BitNet after loading weights from LLMs and performing continued training
(stage-2 in § 3.2), together with their corresponding FP16 distributions.




                (a)                               (b)                               (c)
Figure 3: Analysis of SubLN, layer selection for Eq. 12 and teacher selection over training steps.
(a) Fine-tuning existing LLMs into 1.58-bit BitNet with SubLN yields better performance and faster
convergence. (b) Comparison of MNLI accuracies obtained by distilling different layers on Qwen3-
0.6B. (c) Comparison of MNLI accuracies obtained by distilling Qwen3-0.6B with different size of
FP16 teachers.


nounced. Our experiments reveal that a small amount of continue-training can effectively alleviate
this issue, and here we investigate the underlying reasons.
In Figure 2, we visualize the model weights of 1.58-bit BitNet before and after continue-training,
and compare them with those of a BitNet trained from scratch. We find that after continue-training,
the weight distribution which initially exhibited an approximately Gaussian shape, becomes more
similar to that of a BitNet trained from scratch. This observation supports our hypothesis in § 3.2:
continue-training enables BitNet models to rapidly adapt to the feature space that is better suited for
1.58-bit optimization, thereby preventing convergence to suboptimal local minima and ultimately
leading to improved downstream performance.
Furthermore, we investigate why the BitNet-like weight distribution observed in Figure 2 facilitates
improved performance on downstream tasks. In particular, the unique distribution concentrates more
weights near the transition boundaries between 0 and -1 as well as between 0 and 1. Such placements
allow the quantized values to shift more frequently with small gradient steps, thereby enhancing the
1.58-bit BitNet’s ability to fit downstream data and reducing the risk of being trapped in suboptimal
local minima.
Distillation layer selection strategy in Stage-3 § 3.3. As discussed in § 3.3, we hypothesize that
performing attention relation distillation on a single layer provides the 1.58-bit BitNet with greater
optimization flexibility compared to distilling across all layers, thereby yielding better performance.
To examine this, we explore strategies for selecting the distillation layer. Figure 3 (b) visualizes the
MNLI classification results of Qwen3-0.6B when applying distillation to different layers without


                                                   8
Table 5: Effect of different stages in BitDistill. Here Qwen3-                   Table 6: Effect of distillation
0.6B is used as base model. M.D., C.T., and D.T. denote modeling                 techniques. Here LD denotes
refinement § 3.1, continue pre-training § 3.2, and distillation-based            logits distillation in Eq. 9 and
finetuning § 3.3, respectively.                                                  AD denotes multi-head atten-
                                                                                 tion distillation in Eq. 12.
    Stage-1   Stage-2   Stage-3   MNLI                     CNNDM
     M.D.      C.T.      D.F.     ACC     BLEU    ROUGE-1    ROUGE-2   ROUGE-L       LD       AD       MNLI
      ✗         ✗         ✗       74.09   11.47    37.10      13.97     24.84        ✗         ✗        86.73
      ✓         ✗         ✗       76.30   11.69    37.81      14.13     25.11
      ✓         ✓         ✗       86.73   13.96    39.75      16.47     26.96
                                                                                     ✓         ✗        87.32
      ✓         ✗         ✓       88.04   13.70    39.92      16.91     27.16        ✗         ✓        87.67
      ✓         ✓         ✓       88.17   14.41    40.21      17.47     27.49        ✓         ✓        88.17



continue pre-training. Our findings can be summarized as follows: (1) distilling from a single
layer achieves superior performance compared to using all layers, supporting our hypothesis; (2) the
results vary significantly depending on which single layer is chosen, indicating that an appropriate
layer selection strategy is crucial; and (3) layers located in the later stages of the model tend to
deliver better distillation performance.
Better teacher lead to better results. We investigate whether our proposed BitDistill can lever-
age a higher-quality FP16 teacher to provide greater downstream task gains for the 1.58-bit BitNet.
To this end, we use Qwen3-1.7B and Qwen3-4B FP16 models as teachers in the distillation process
for the Qwen3-0.6B 1.58-bit BitNet. The results are visualized in Figure 3 (c). We find that our algo-
rithm can effectively extract larger gains from a higher-quality teacher, even surpassing FP16 models
of the same size. This provides a performance guarantee for deploying BitNet models tailored to
specific tasks.

5      Related Work
Quantization for LLMs Quantization [TXL+ 25, DZC+ 24, MWM+ 24] has emerged as a widely
adopted technique for enhancing the efficiency and scalability of LLMs. Post-training quantiza-
tion (PTQ) [XLS+ 23, DLBZ22] like GPTQ [FAHA22] and AWQ [LTT+ 24] has been extensively
studied for weight-only quantization of LLMs. PTQ applies low-bit quantization to a full-precision
model using a small set of calibration data, without requiring access to the end-to-end training loss.
However, PTQ always suffer from significant performance degradation, especially when quantiza-
tion bits are lower than 4 bits [DLBZ22]. To address this limitation, quantization-aware training
(QAT) [TXL+ 25, LOZ+ 23, CSX+ 24] has been introduced, which continues training the quantized
LLMs with sufficient optimization, thereby raising the performance ceiling achievable by quantized
models.
Knowledge Distillation for LLMs Knowledge distillation [KKCY24, HVD15, WWD+ 20,
TXL+ 25] has proven to be an effective technique for compressing large language models (LLMs)
while preserving accuracy, by transferring knowledge from a high-capacity teacher model to a more
compact student model. More recently, it has also been shown effective for transferring knowledge
from full-precision models to quantized LLMs. For example, TSLD [KLL+ 23] employs layer-
to-layer distillation to enhance quantization-aware training (QAT) for ternary quantization, while
BitDistiller [DZC+ 24] leverages self-distillation to improve the performance of LLMs at ultra-low
precisions (e.g., 2 or 3 bits). Despite these advances, most existing methods primarily target gen-
eral language modeling capabilities and still exhibit noticeable performance gaps in downstream
applications compared to their full-precision counterparts.

6      Conclusion
In this work, we investigated the problem of adapting pre-trained LLMs to ultra-low precision with
only 1.58-bit weights, motivated by the practical need to deploy large-scale models on edge devices
under strict memory and latency constraints. To this end, we introduced BitNet Distillation,
a three-stage framework that first performs model refinement with SubLN, and then continued pre-
training to recover critical representation capacity, followed by knowledge distillation at both the
hidden-state and attention-relation levels to narrow the accuracy gap between low-precision students


                                                             9
and high-precision teachers. Extensive experiments on multiple downstream tasks demonstrate that
our method, BitDistill, achieves performance competitive with FP16 models while significantly
reducing the computational and memory footprint. Beyond improving efficiency, our approach pro-
vides new insights into how low-bit quantization interacts with both pretraining and distillation
dynamics, shedding light on scalable strategies for resource-constrained deployment.

References
 [AAA+ 23] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-
           cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anad-
           kat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
   [BLC13] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagat-
           ing gradients through stochastic neurons for conditional computation. arXiv preprint
           arXiv:1308.3432, 2013.
 [BMH+ 22] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford,
           Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc,
           Aidan Clark, et al. Improving language models by retrieving from trillions of tokens,
           2022.
 [CSX+ 24] Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang,
           and Ping Luo. Efficientqat: Efficient quantization-aware training for large language
           models. arXiv preprint arXiv:2407.11062, 2024.
 [DLBZ22] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 ():
          8-bit matrix multiplication for transformers at scale. Advances in neural information
          processing systems, 35:30318–30332, 2022.
  [DLSZ21] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via
           block-wise quantization. arXiv preprint arXiv:2110.02861, 2021.
 [DZC+ 24] Dayou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen Chu, and Ningyi
           Xu. Bitdistiller: Unleashing the potential of sub-4-bit llms via self-distillation. arXiv
           preprint arXiv:2402.10631, 2024.
 [FAHA22] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accu-
          rate post-training quantization for generative pre-trained transformers. arXiv preprint
          arXiv:2210.17323, 2022.
 [GYZ+ 25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu,
           Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing rea-
           soning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948,
           2025.
 [HKG+ 15] Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will
           Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and compre-
           hend. In NIPS, pages 1693–1701, 2015.
  [HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural
          network, 2015.
 [HZL+ 24] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley,
           and Wayne Xin Zhao. Large language models are zero-shot rankers for recommender
           systems. In European Conference on Information Retrieval, pages 364–381. Springer,
           2024.
  [KDSP25] Arina Kostina, Marios D Dikaiakos, Dimosthenis Stefanidis, and George Pallis. Large
           language models for text classification: Case study and comprehensive review. arXiv
           preprint arXiv:2501.08457, 2025.
 [KKCY24] Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. Distillm: Towards
          streamlined distillation for large language models. arXiv preprint arXiv:2402.03898,
          2024.


                                                10
 [KLL+ 23] Minsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong Chang, Wony-
           ong Sung, and Jungwook Choi. Token-scaled logit distillation for ternary weight
           generative language models. Advances in Neural Information Processing Systems,
           36:42097–42118, 2023.
    [Lin04] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text
            summarization branches out, pages 74–81, 2004.
 [LOZ+ 23] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar
           Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-
           qat: Data-free quantization aware training for large language models. arXiv preprint
           arXiv:2305.17888, 2023.
 [LSK+ 25] Jung Hyun Lee, Seungjae Shin, Vinnam Kim, Jaeseong You, and An Chen. Unify-
           ing block-wise ptq and distillation-based qat for progressive quantization toward 2-bit
           instruction-tuned llms. arXiv preprint arXiv:2506.09104, 2025.
  [LTT+ 24] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang,
            Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware
            weight quantization for on-device llm compression and acceleration. Proceedings of
            machine learning and systems, 6:87–100, 2024.
[MWH+ 25] Shuming Ma, Hongyu Wang, Shaohan Huang, Xingxing Zhang, Ying Hu, Ting
          Song, Yan Xia, and Furu Wei. Bitnet b1. 58 2b4t technical report. arXiv preprint
          arXiv:2504.12285, 2025.
[MWM+ 24] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan
          Huang, Lifeng Dong, Ruiping Wang, Jilong Xue, and Furu Wei. The era of 1-bit
          llms: All large language models are in 1.58 bits. arXiv preprint arXiv:2402.17764,
          1(4), 2024.
 [PMH+ 23] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessan-
           dro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien
           Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with
           web data, and web data only, 2023.
 [PRWZ02] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method
          for automatic evaluation of machine translation. In Proceedings of the 40th annual
          meeting of the Association for Computational Linguistics, pages 311–318, 2002.
  [QY+ 25] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen
           Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jian-
           hong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai
           Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang,
           Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren,
           Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu
           Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025.
 [RWX+ 24] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei
           Yin, and Chao Huang. Representation learning with large language models for rec-
           ommendation. In Proceedings of the ACM web conference 2024, pages 3464–3475,
           2024.
  [RZLL16] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,
           000+ questions for machine comprehension of text. CoRR, abs/1606.05250, 2016.
  [SLL+ 23] Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and
            Guoyin Wang. Text classification via large language models. arXiv preprint
            arXiv:2305.08377, 2023.
 [SPW+ 13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning,
           Andrew Ng, and Christopher Potts. Recursive deep models for semantic composition-
           ality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical


                                               11
             Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington,
             USA, October 2013. Association for Computational Linguistics.
 [TKF+ 25] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ra-
           mona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière,
           et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025.
 [TXL+ 25] MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian
           Chen, Wentong Chen, Xin Cong, Ganqu Cui, et al. Minicpm4: Ultra-efficient llms on
           end devices. arXiv preprint arXiv:2506.07900, 2025.
[WBH+ 20] Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. Minilmv2:
          Multi-head self-attention relation distillation for compressing pretrained transformers.
          arXiv preprint arXiv:2012.15828, 2020.
[WMD+ 23] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma,
          Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers
          for large language models. arXiv preprint arXiv:2310.11453, 2023.
  [WNB18] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge
          corpus for sentence understanding through inference. In Proceedings of the 2018
          Conference of the North American Chapter of the Association for Computational
          Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–
          1122. Association for Computational Linguistics, 2018.
[WSM+ 19] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.
          Bowman. GLUE: A multi-task benchmark and analysis platform for natural language
          understanding. In International Conference on Learning Representations, 2019.
[WWD+ 20] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm:
          Deep self-attention distillation for task-agnostic compression of pre-trained transform-
          ers. Advances in neural information processing systems, 33:5776–5788, 2020.
[WZQ+ 24] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen,
          Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. A survey on large language models
          for recommendation. World Wide Web, 27(5):60, 2024.
 [XLS+ 23] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
           Smoothquant: Accurate and efficient post-training quantization for large language
           models. In International conference on machine learning, pages 38087–38099. PMLR,
           2023.
 [YLY+ 25] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen
           Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv
           preprint arXiv:2505.09388, 2025.
 [ZSB+ 24] Yiyun Zhao, Prateek Singh, Hanoz Bhathena, Bernardo Ramos, Aviral Joshi, Swa-
           roop Gadiyaram, and Saket Sharma. Optimizing llm based retrieval augmented gener-
           ation pipelines in the financial domain. In Proceedings of the 2024 Conference of the
           North American Chapter of the Association for Computational Linguistics: Human
           Language Technologies (Volume 6: Industry Track), pages 279–294, 2024.




                                               12
