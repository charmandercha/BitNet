---
library_name: transformers
tags:
- translation
language:
- zh
- en
- fr
- pt
- es
- ja
- tr
- ru
- ar
- ko
- th
- it
- de
- vi
- ms
- id
- tl
- hi
- pl
- cs
- nl
- km
- my
- fa
- gu
- ur
- te
- mr
- he
- bn
- ta
- uk
- bo
- kk
- mn
- ug
---


<p align="center">
 <img src="https://github.com/Tencent-Hunyuan/HY-MT/raw/main/imgs/hunyuanlogo.png" width="400"/> <br>
</p><p></p>


<p align="center">
    ğŸ¤—&nbsp;<a href="https://huggingface.co/collections/tencent/hy-mt15"><b>Hugging Face</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;
    ğŸ•¹ï¸&nbsp;<a href="https://hunyuan.tencent.com/chat/HunyuanDefault?from=modelSquare&modelId=hunyuan-mt-1.8b"><b>Demo</b></a>&nbsp;&nbsp;&nbsp;&nbsp;
    ğŸ¤–&nbsp;<a href="https://modelscope.cn/collections/Tencent-Hunyuan/HY-MT15"><b>ModelScope</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;
</p>

<p align="center">
    ğŸ–¥ï¸&nbsp;<a href="https://hunyuan.tencent.com"><b>Official Website</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;
    <a href="https://github.com/Tencent-Hunyuan/HY-MT"><b>Github</b></a>
</p>


## Model Introduction

Hunyuan Translation Model Version 1.5 includes a 1.8B translation model, HY-MT1.5-1.8B, and a 7B translation model, HY-MT1.5-7B. Both models focus on supporting mutual translation across 33 languages and incorporating 5 ethnic and dialect variations. Among them, HY-MT1.5-7B is an upgraded version of our WMT25 championship model, optimized for explanatory translation and mixed-language scenarios, with newly added support for terminology intervention, contextual translation, and formatted translation. Despite having less than one-third the parameters of HY-MT1.5-7B, HY-MT1.5-1.8B delivers translation performance comparable to its larger counterpart, achieving both high speed and high quality. After quantization, the 1.8B model can be deployed on edge devices and support real-time translation scenarios, making it widely applicable.

## Key Features and Advantages

- HY-MT1.5-1.8B achieves the industry-leading performance among models of the same size, surpassing most commercial translation APIs.
- HY-MT1.5-1.8B supports deployment on edge devices and real-time translation scenarios, offering broad applicability.
- HY-MT1.5-7B, compared to its September open-source version, has been optimized for annotated and mixed-language scenarios.
- Both models support terminology intervention, contextual translation, and formatted translation.

## Related News
* 2025.12.30, we have open-sourced **HY-MT1.5-1.8B** and **HY-MT1.5-7B** on Hugging Face.
* 2025.9.1, we have open-sourced  **Hunyuan-MT-7B** , **Hunyuan-MT-Chimera-7B** on Hugging Face.
<br>


## Performance

<div align='center'>
<img src="https://github.com/Tencent-Hunyuan/HY-MT/raw/main/imgs/overall_performance.png" width = "100%" />
</div>
You can refer to our technical report for more experimental results and analysis.

<a href="https://github.com/Tencent-Hunyuan/HY-MT/raw/main/HY_MT1_5_Technical_Report.pdf"><b>Technical Report</b> </a>

&nbsp;

## Model Links
| Model Name  | Description | Download |
| ----------- | ----------- |-----------
| HY-MT1.5-1.8B  | Hunyuan 1.8B translation model |ğŸ¤— [Model](https://huggingface.co/tencent/HY-MT1.5-1.8B)|
| HY-MT1.5-1.8B-FP8 | Hunyuan 1.8B translation model, fp8 quant    | ğŸ¤— [Model](https://huggingface.co/tencent/HY-MT1.5-1.8B-FP8)|
| HY-MT1.5-1.8B-GPTQ-Int4 | Hunyuan 1.8B translation model, int4 quant    | ğŸ¤— [Model](https://huggingface.co/tencent/HY-MT1.5-1.8B-GPTQ-Int4)|
| HY-MT1.5-7B | Hunyuan 7B translation model    | ğŸ¤— [Model](https://huggingface.co/tencent/HY-MT1.5-7B)|
| HY-MT1.5-7B-FP8 | Hunyuan 7B translation model, fp8 quant     | ğŸ¤— [Model](https://huggingface.co/tencent/HY-MT1.5-7B-FP8)|
| HY-MT1.5-7B-GPTQ-Int4 | Hunyuan 7B translation model, int4 quant     | ğŸ¤— [Model](https://huggingface.co/tencent/HY-MT1.5-7B-GPTQ-Int4)|

## Prompts

### Prompt Template for ZH<=>XX Translation.
---
```
å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸º{target_language}ï¼Œæ³¨æ„åªéœ€è¦è¾“å‡ºç¿»è¯‘åçš„ç»“æœï¼Œä¸è¦é¢å¤–è§£é‡Šï¼š

{source_text}
```
---

### Prompt Template for XX<=>XX Translation, excluding ZH<=>XX.
---
```
Translate the following segment into {target_language}, without additional explanation.

{source_text}
```
---

### Prompt Template for terminology intervention.
---
```
å‚è€ƒä¸‹é¢çš„ç¿»è¯‘ï¼š
{source_term} ç¿»è¯‘æˆ {target_term}

å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸º{target_language}ï¼Œæ³¨æ„åªéœ€è¦è¾“å‡ºç¿»è¯‘åçš„ç»“æœï¼Œä¸è¦é¢å¤–è§£é‡Šï¼š
{source_text}
```
---

### Prompt Template for contextual translation.
---
```
{context}
å‚è€ƒä¸Šé¢çš„ä¿¡æ¯ï¼ŒæŠŠä¸‹é¢çš„æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ï¼Œæ³¨æ„ä¸éœ€è¦ç¿»è¯‘ä¸Šæ–‡ï¼Œä¹Ÿä¸è¦é¢å¤–è§£é‡Šï¼š
{source_text}

```
---

###  Prompt Template for formatted translation.
---
```
å°†ä»¥ä¸‹<source></source>ä¹‹é—´çš„æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œæ³¨æ„åªéœ€è¦è¾“å‡ºç¿»è¯‘åçš„ç»“æœï¼Œä¸è¦é¢å¤–è§£é‡Šï¼ŒåŸæ–‡ä¸­çš„<sn></sn>æ ‡ç­¾è¡¨ç¤ºæ ‡ç­¾å†…æ–‡æœ¬åŒ…å«æ ¼å¼ä¿¡æ¯ï¼Œéœ€è¦åœ¨è¯‘æ–‡ä¸­ç›¸åº”çš„ä½ç½®å°½é‡ä¿ç•™è¯¥æ ‡ç­¾ã€‚è¾“å‡ºæ ¼å¼ä¸ºï¼š<target>str</target>

<source>{src_text_with_format}</source>
```
---

&nbsp;

### Use with transformers
First, please install transformers, recommends v4.56.0
```SHELL
pip install transformers==4.56.0
```

*!!! If you want to load fp8 model with transformers, you need to change the name"ignored_layers" in config.json to "ignore" and upgrade the compressed-tensors to compressed-tensors-0.11.0.*

The following code snippet shows how to use the transformers library to load and apply the model.

we use tencent/HY-MT1.5-1.8B for example

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

model_name_or_path = "tencent/HY-MT1.5-1.8B"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map="auto")  # You may want to use bfloat16 and/or move to GPU here
messages = [
    {"role": "user", "content": "Translate the following segment into Chinese, without additional explanation.\n\nItâ€™s on the house."},
]
tokenized_chat = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=False,
    return_tensors="pt"
)

outputs = model.generate(tokenized_chat.to(model.device), max_new_tokens=2048)
output_text = tokenizer.decode(outputs[0])
```

We recommend using the following set of parameters for inference. Note that our model does not have the default system_prompt.

```json
{
  "top_k": 20,
  "top_p": 0.6,
  "repetition_penalty": 1.05,
  "temperature": 0.7
}
```

&nbsp;

Supported languages:
| Languages         | Abbr.   | Chinese Names   |
|-------------------|---------|-----------------|
| Chinese           | zh      | ä¸­æ–‡            |
| English           | en      | è‹±è¯­            |
| French            | fr      | æ³•è¯­            |
| Portuguese        | pt      | è‘¡è„ç‰™è¯­        |
| Spanish           | es      | è¥¿ç­ç‰™è¯­        |
| Japanese          | ja      | æ—¥è¯­            |
| Turkish           | tr      | åœŸè€³å…¶è¯­        |
| Russian           | ru      | ä¿„è¯­            |
| Arabic            | ar      | é˜¿æ‹‰ä¼¯è¯­        |
| Korean            | ko      | éŸ©è¯­            |
| Thai              | th      | æ³°è¯­            |
| Italian           | it      | æ„å¤§åˆ©è¯­        |
| German            | de      | å¾·è¯­            |
| Vietnamese        | vi      | è¶Šå—è¯­          |
| Malay             | ms      | é©¬æ¥è¯­          |
| Indonesian        | id      | å°å°¼è¯­          |
| Filipino          | tl      | è²å¾‹å®¾è¯­        |
| Hindi             | hi      | å°åœ°è¯­          |
| Traditional Chinese | zh-Hant| ç¹ä½“ä¸­æ–‡        |
| Polish            | pl      | æ³¢å…°è¯­          |
| Czech             | cs      | æ·å…‹è¯­          |
| Dutch             | nl      | è·å…°è¯­          |
| Khmer             | km      | é«˜æ£‰è¯­          |
| Burmese           | my      | ç¼…ç”¸è¯­          |
| Persian           | fa      | æ³¢æ–¯è¯­          |
| Gujarati          | gu      | å¤å‰æ‹‰ç‰¹è¯­      |
| Urdu              | ur      | ä¹Œå°”éƒ½è¯­        |
| Telugu            | te      | æ³°å¢å›ºè¯­        |
| Marathi           | mr      | é©¬æ‹‰åœ°è¯­        |
| Hebrew            | he      | å¸Œä¼¯æ¥è¯­        |
| Bengali           | bn      | å­ŸåŠ æ‹‰è¯­        |
| Tamil             | ta      | æ³°ç±³å°”è¯­        |
| Ukrainian         | uk      | ä¹Œå…‹å…°è¯­        |
| Tibetan           | bo      | è—è¯­            |
| Kazakh            | kk      | å“ˆè¨å…‹è¯­        |
| Mongolian         | mn      | è’™å¤è¯­          |
| Uyghur            | ug      | ç»´å¾å°”è¯­        |
| Cantonese         | yue     | ç²¤è¯­            |


Citing HY-MT1.5:

```bibtex
@misc{hy-mt1.5,
      title={HY-MT1.5 Technical Report}, 
      author={Mao Zheng and Zheng Li and Tao Chen and Mingyang Song and Di Wang},
      year={2025},
      eprint={2512.24092},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2512.24092}, 
}
```